## 初投稿時に考えたこと

将棋AIを開発するプロジェクトを開始しました。

たしか開始宣言を行ったのは1週間くらい前（2020-08-15）だった気がします。

凡人は今でこそAIの研究を仕事で行っていますが、元々はコンピュータサイエンス上がりのHPC (High Performance Computing)の研究者でした。
そうゆう経緯もあり、日々精進はしているものの、AIについてはまだまだ素人研究者です。
ということで、この将棋AIを開発するというプロジェクトは、単にYouTubeのネタになるだけではなく、AI研究開発のスキルアップも図れるので一石二鳥なのです。
いや、今回このGitHubのアカウントも持つことになったので、IT界隈での活動領域を拡大できたという意味では一石三鳥なのかもしれません。いいことづくめです。

元々、仕事以外でも勉強する意欲はあったのですが、なかなか重い腰があがらず行動のレベルにまで昇華することはありませんでした。
行動を移すきっかけとなったイベントはいくつあったのですが、昨今の将棋ブーム（藤井聡太棋士の貢献が大きい）が強くドライブさせてくれている気がします。
将棋ブームは、YouTube的にはもちろん追い風ですし、ホットな話題についての開発は技術者としてはとてもワクワクします。

## 回顧してみると
Atariというレトロゲームで人間を超えたプレイを実現したという内容の[論文](https://www.nature.com/articles/nature14236)が発表された際（2015年頃）に、まだAI界隈にいなかった私はとても興味を持ちました。
素人ながらもワクワクしたことを今でも覚えています。
凡人にとって衝撃的だったのは、[深層強化学習](https://ledge.ai/reinforcement-learning/)という機械学習の手法を用いて実現したところです。
学習エージェント（AI）が自身でゲームを行うのですが、ゲームでの得点を元に良いプレイの仕方を勝手に学ぶというのです。

この技術を研究開発したDeep Mind社（現在Googleグループ傘下）は、この1年後の2016年に、同じく深層強化学習を用いて[AlphaGo](https://ja.wikipedia.org/wiki/AlphaGo)という名の囲碁AIを開発して、世界で初めてプロの囲碁棋士（当時最強と呼ばれた[イ・セドル九段](https://ja.wikipedia.org/wiki/%E6%9D%8E%E4%B8%96%E3%83%89%E3%83%AB)）に5戦して4勝したのです。
この出来事はAIの歴史で3本指に入る重大なイベントで、世界中のAI研究者は当然として凡人にとっても大変衝撃的でした。
その後1年の間にさらなる研究開発が行われて、AlphaGo → [AlphaGo Master](https://ja.wikipedia.org/wiki/Master) → [AlphaZero](https://science.sciencemag.org/content/362/6419/1140) とレベルアップし、2017年時点で僅か3日間の学習で当時最強のチェスAI、将棋AI、囲碁AIを撃破しました。
人類史が塗り替えられることが短期間の間に次々と起こったのです。AlphaGo筆頭の研究者David Silverは、その多大な功績により、どこかのコミュニティから1億円ほど賞金をもらったとか。
この一連の流れをリアルタイムで見てきたのですが、ロマンのある研究だなぁと常々感じていました。

深層強化学習については、ロマンとワクワクを感じつつも、技術目線で厳しく見ると実用上の課題があまりに多く山積していると言わざるを得ないため、仕事にはまだまだ活かせないなぁとも感じていました。
一方で、ここ1-2年で深層強化学習を取り扱うことがことができる[ソフトウェアプラットフォーム](https://gym.openai.com/)が充実してきたので、趣味レベルでやってみようかなぁと思っていました。

## 今週の進捗
- GitHubアカウント作成
- 使えそうな将棋ソフトの調査
- 同じく将棋AIを開発している人達のブログを拝見


他人の技術ブログを見て将棋を深層強化学習で強くするのはかなり高難易度なんだと判明してきました。
多方面からあらゆるチューニングを行っている印象です。ゴリゴリのチューニングなしで行けるか自信をなくしました。。。

深層強化学習に使えそうな将棋ソフトをいくつか見つけることができました。
ブログを見る限り、将棋ソフトの処理スピードは重要なようなので、学習用と対人プレイ用でソフトを分けて使うかたちになりそうです。

将棋は囲碁と違って、持ち駒というシステムがあるため、エンコーディング（盤面の状態／指し方を数値的にどのように表すか）が難しい。
経験上、AlphaGoやブログで紹介されている方法は、将棋の空間的or意味的な関係性を使えず状態解釈についての非線形性が強くなりすぎている気がします。
従来の汎用的なエンコーディングだと[ニューラルネットワーク](https://ledge.ai/neural-network/)を深くすることが必要となって、ネットワークの更新に使用するデータが大量に必要となってしまいます。
これはプレイアウト数の増大、すなわち、学習に膨大な時間がかかってしまうことにつながるので、Googleのように膨大な計算リソースを持たないとまともに学習ができません。
凡人は大した計算リソースを持っていないので、ある程度きちんとエンコーディングしてあげる必要があると戒めておきます。

## 現時点での方針・ToDo

とにかく学習に時間がかかる機雷があるので、積極的にチートしたいところです。
既存の将棋AIは盤面を基にどちらのプレイヤーが有利であるかスコア化してくれるとのことなので、これをうまく使って学習させる作戦です。
模倣学習から始めて徐々に強化学習に移行していくカリキュラム学習を行うという学習フレームワークが良いのではないかと考えています。
上手く移行させるのは難しそうな気がするので、ここが技術者としての腕の見せ所かもしれないです。
気を付けなくてはいけないのは、既存の将棋AIのコピーにならないようにすることです。既存の将棋AIに引き分けではダメで、勝てるようにしていきたいです。

また凡人の直感では[モンテカルロ木探索（MCTS: Monte Carlo Tree Search）](https://ja.wikipedia.org/wiki/%E3%83%A2%E3%83%B3%E3%83%86%E3%82%AB%E3%83%AB%E3%83%AD%E6%9C%A8%E6%8E%A2%E7%B4%A2)については、将棋に限って言えば、停留点を多数つくってしまい学習が進まくなる一因になるのではと思います。
MCTSの計算量は膨大なのですが、将棋ルール下の深層強化学習にとって計算量に見合うメリットがあるのか疑問です。なので、凡人は別の軽量なアプローチを試してみたいと思います。最終的にMCTSを採用するオチもありますが。


次週は、将棋ソフトと活用する既存の将棋AIについて調査と選定を行う予定です。
